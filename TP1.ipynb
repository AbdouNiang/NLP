{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "834551d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "267e4cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=pd.read_csv(\"twitter-2013train-A.txt\",sep='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "582149c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>264183816548130816</td>\n",
       "      <td>positive</td>\n",
       "      <td>Gas by my house hit $3.39!!!! I\\u2019m going t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263405084770172928</td>\n",
       "      <td>negative</td>\n",
       "      <td>Theo Walcott is still shit\\u002c watch Rafa an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262163168678248449</td>\n",
       "      <td>negative</td>\n",
       "      <td>its not that I\\u2019m a GSP fan\\u002c i just h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264249301910310912</td>\n",
       "      <td>negative</td>\n",
       "      <td>Iranian general says Israel\\u2019s Iron Dome c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>262682041215234048</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Tehran\\u002c Mon Amour: Obama Tried to Establi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0         1  \\\n",
       "0  264183816548130816  positive   \n",
       "1  263405084770172928  negative   \n",
       "2  262163168678248449  negative   \n",
       "3  264249301910310912  negative   \n",
       "4  262682041215234048   neutral   \n",
       "\n",
       "                                                   2  \n",
       "0  Gas by my house hit $3.39!!!! I\\u2019m going t...  \n",
       "1  Theo Walcott is still shit\\u002c watch Rafa an...  \n",
       "2  its not that I\\u2019m a GSP fan\\u002c i just h...  \n",
       "3  Iranian general says Israel\\u2019s Iron Dome c...  \n",
       "4  Tehran\\u002c Mon Amour: Obama Tried to Establi...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25e2032f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5917</th>\n",
       "      <td>101786196639883264</td>\n",
       "      <td>positive</td>\n",
       "      <td>Vampire Diaries and Jersey Shore tonight. So f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9211</th>\n",
       "      <td>251154711175106560</td>\n",
       "      <td>positive</td>\n",
       "      <td>@crazycharmx3 i love doing that\\u002c hey we a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>221444699322458115</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Today was National Kissing Day\\u002c I\\u2019ll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7298</th>\n",
       "      <td>101194901470986240</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@LaurenScottyXO oh there is no school today be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6257</th>\n",
       "      <td>111326660908945408</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@nidge1uk Yer off to take son for his first da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4068</th>\n",
       "      <td>264155923780612096</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@ArinRodriguez I texted you on Tuesday I was T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4454</th>\n",
       "      <td>259397264504664064</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Road Trip (sort of) with my boyfriend this Mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>246358852092694529</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@ShutDownTheSun \\\"\"these birds dont buy the su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5803</th>\n",
       "      <td>100230511959605248</td>\n",
       "      <td>positive</td>\n",
       "      <td>@harryrippon10 yeah United were quality though...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474</th>\n",
       "      <td>263074918260961280</td>\n",
       "      <td>positive</td>\n",
       "      <td>Wish FIFA would make league results and tables...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5781</th>\n",
       "      <td>100076597658390528</td>\n",
       "      <td>positive</td>\n",
       "      <td>RT @deltaupsilon: Boise State, Chattanooga, Em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6385</th>\n",
       "      <td>100017370185220096</td>\n",
       "      <td>positive</td>\n",
       "      <td>RT @CGBlasi: Very impressed with Joco Teenage ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>264091046722560000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>My Thursday nights aren\\u2019t the same with W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8138</th>\n",
       "      <td>235616936795467777</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@mahesha13 Ying wants to know if you still des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>264131501376040961</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@CASS_TAR is addicted to ACT. She just took on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3657</th>\n",
       "      <td>262959496035459072</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Jennifer Aniston takes the plunge is low-cut\\u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>251230219342602240</td>\n",
       "      <td>neutral</td>\n",
       "      <td>We\\u2019re thinking of running on the Australi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>262254456400392192</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@BethFishReads Do you have chocolate? Sunday i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8212</th>\n",
       "      <td>259821602097737730</td>\n",
       "      <td>negative</td>\n",
       "      <td>@KelBel_x0 1st and 10\\u002c ball on the WSH 34...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3054</th>\n",
       "      <td>256109715463995392</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Photos: The 10th Hour &amp; Fear of None played th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0         1  \\\n",
       "5917  101786196639883264  positive   \n",
       "9211  251154711175106560  positive   \n",
       "867   221444699322458115   neutral   \n",
       "7298  101194901470986240   neutral   \n",
       "6257  111326660908945408   neutral   \n",
       "4068  264155923780612096   neutral   \n",
       "4454  259397264504664064   neutral   \n",
       "4998  246358852092694529   neutral   \n",
       "5803  100230511959605248  positive   \n",
       "1474  263074918260961280  positive   \n",
       "5781  100076597658390528  positive   \n",
       "6385  100017370185220096  positive   \n",
       "608   264091046722560000   neutral   \n",
       "8138  235616936795467777   neutral   \n",
       "957   264131501376040961   neutral   \n",
       "3657  262959496035459072   neutral   \n",
       "557   251230219342602240   neutral   \n",
       "1076  262254456400392192   neutral   \n",
       "8212  259821602097737730  negative   \n",
       "3054  256109715463995392   neutral   \n",
       "\n",
       "                                                      2  \n",
       "5917  Vampire Diaries and Jersey Shore tonight. So f...  \n",
       "9211  @crazycharmx3 i love doing that\\u002c hey we a...  \n",
       "867   Today was National Kissing Day\\u002c I\\u2019ll...  \n",
       "7298  @LaurenScottyXO oh there is no school today be...  \n",
       "6257  @nidge1uk Yer off to take son for his first da...  \n",
       "4068  @ArinRodriguez I texted you on Tuesday I was T...  \n",
       "4454  Road Trip (sort of) with my boyfriend this Mon...  \n",
       "4998  @ShutDownTheSun \\\"\"these birds dont buy the su...  \n",
       "5803  @harryrippon10 yeah United were quality though...  \n",
       "1474  Wish FIFA would make league results and tables...  \n",
       "5781  RT @deltaupsilon: Boise State, Chattanooga, Em...  \n",
       "6385  RT @CGBlasi: Very impressed with Joco Teenage ...  \n",
       "608   My Thursday nights aren\\u2019t the same with W...  \n",
       "8138  @mahesha13 Ying wants to know if you still des...  \n",
       "957   @CASS_TAR is addicted to ACT. She just took on...  \n",
       "3657  Jennifer Aniston takes the plunge is low-cut\\u...  \n",
       "557   We\\u2019re thinking of running on the Australi...  \n",
       "1076  @BethFishReads Do you have chocolate? Sunday i...  \n",
       "8212  @KelBel_x0 1st and 10\\u002c ball on the WSH 34...  \n",
       "3054  Photos: The 10th Hour & Fear of None played th...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5684e742",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.columns=['id','sentiment','text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a20d48df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test=pd.read_csv(\"twitter-2013test-A.txt\",sep='\\t',header=None)\n",
    "data_test.columns=['id','sentiment','text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72ea5e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dev=pd.read_csv(\"twitter-2013dev-A.txt\",sep='\\t',header=None)\n",
    "data_dev.columns=['id','sentiment','text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecdf4ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Niang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Niang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d89f052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bonjou', 'comment', 'vous', 'allez']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(\"bonjou comment vous allez\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d8ed81",
   "metadata": {},
   "source": [
    "# Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9510fe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8e2100",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46a29463",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there\n",
      "are\n",
      "sever\n",
      "type\n",
      "of\n",
      "stem\n",
      "algorithm\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "input_str=\"There are several types of stemming algorithm.\"\n",
    "input_str= nltk.word_tokenize(input_str)\n",
    "for word in input_str :\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c337f04",
   "metadata": {},
   "source": [
    "# Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0a2fc51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "been\n",
      "had\n",
      "done\n",
      "language\n",
      "city\n",
      "nice\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "input_str=\"been had done languages cities nice \"\n",
    "input_str= nltk.word_tokenize(input_str)\n",
    "for word in input_str :\n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13267c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a8bdad",
   "metadata": {},
   "source": [
    "# Cette fonction regroupe les pretraitements du premier de mon corpus et sera généralisé dans l\"emsemble du corpus par la fonction composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17535aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraire_train(data):\n",
    "        lemmatizer=WordNetLemmatizer()\n",
    "        for index,row in data.iterrows():\n",
    "            dictionnaire={}\n",
    "            filter_sentence=[]\n",
    "            i=1\n",
    "            sentence=row['text']\n",
    "            sentence = re.sub(r'[^\\w\\s]','',sentence)   #cleaning \n",
    "            words= nltk.word_tokenize(sentence)    # tokonization\n",
    "            words =[w for w in words if not w in stop_words] #stopwords\n",
    "            for word in words:\n",
    "                filter_sentence.append(lemmatizer.lemmatize(word)) #lemmatizer\n",
    "            #print(filter_sentence)\n",
    "            for val in filter_sentence:\n",
    "                    dictionnaire[val]=i\n",
    "                    i+=1\n",
    "            return dictionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b14a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict=extraire_train(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b23eea1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22953402",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict_test=extraire_train(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f149d153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_dict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5031b18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict_dev=extraire_train(data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f0dce0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_dict_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ac4d4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a25f96a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def composition_train(data):\n",
    "    ex=extraire_train(data)\n",
    "    dictionnaire={}\n",
    "    final_dictionnaire={}\n",
    "    for item in range(0,(data.shape[0])):\n",
    "        for i,val in ex.items():\n",
    "            dictionnaire[i]=0\n",
    "        for frequent in word_tokenize(data.iloc[item,-1]):\n",
    "            if frequent in list(dictionnaire.keys()):\n",
    "                dictionnaire[frequent]+=1\n",
    "        if data.iloc[item,1]=='positive':\n",
    "            dictionnaire['label']=2\n",
    "        elif data.iloc[item,1]=='negative':\n",
    "             dictionnaire['label']=1\n",
    "        else:\n",
    "            dictionnaire['label']=0\n",
    "            \n",
    "        final_dictionnaire[data.iloc[item,0]]=tuple(dictionnaire.values())\n",
    "    matrice=np.array([final_dictionnaire[index] for index in final_dictionnaire.keys() ])\n",
    "    return matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "776a99c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=composition_train(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c073950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ... 1 1 2]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 2]\n",
      " [0 0 0 ... 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad48fdf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9649"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fd773da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2 ... 1 1 2]\n",
      " [0 0 0 ... 0 0 2]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 1 1]\n",
      " [0 0 0 ... 0 1 2]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "3545\n"
     ]
    }
   ],
   "source": [
    "test=composition_train(data_test)\n",
    "print(test)\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a641c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ... 1 1 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "1649\n"
     ]
    }
   ],
   "source": [
    "dev=composition_train(data_dev)\n",
    "print(dev)\n",
    "print(len(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6112bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"train.svm\", \"w\")\n",
    "for i in range(train.shape[0]):\n",
    "    file.write(str(train[i,-1]))\n",
    "    file.write(' ')\n",
    "    for j in range(train.shape[1]-1):\n",
    "        file.write(str(j+1)+':'+str(train[i,j]))\n",
    "        file.write(' ')\n",
    "    file.write('\\n')        \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d67d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"test.svm\", \"w\")\n",
    "for i in range(test.shape[0]):\n",
    "    file.write(str(test[i,-1]))\n",
    "    file.write(' ')\n",
    "    for j in range(test.shape[1]-1):\n",
    "        file.write(str(j+1)+':'+str(test[i,j]))\n",
    "        file.write(' ')\n",
    "    file.write('\\n')        \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d705bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"dev.svm\", \"w\")\n",
    "for i in range(dev.shape[0]):\n",
    "    file.write(str(train[i,-1]))\n",
    "    file.write(' ')\n",
    "    for j in range(dev.shape[1]-1):\n",
    "        file.write(str(j+1)+':'+str(dev[i,j]))\n",
    "        file.write(' ')\n",
    "    file.write('\\n')        \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d006610",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
